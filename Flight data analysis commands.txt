
1. Create two instances: namenode and datanode. 

##################################################
jdk, hadoop installation on namenode and datanodes
##################################################

1. Install jdk-8:
   sudo apt-get update
   sudo apt-get install openjdk-8-jdk

2. Download and install hadoop-2.6.0
   wget http://apache.mirrors.tds.net/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz -P ~/Downloads
   sudo tar zxvf ~/Downloads/hadoop-* -C /usr/local
   sudo mv /usr/local/hadoop-* /usr/local/hadoop

3. Environmental Variables

   Edit ~/.profile as follows:

   export JAVA_HOME=/usr
   export PATH=$PATH:$JAVA_HOME/bin
   export HADOOP_HOME=/usr/local/hadoop
   export PATH=$PATH:$HADOOP_HOME/bin
   export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop

4. Hadoop Configurations common on all nodes

   (a) $HADOOP_CONF_DIR/hadoop-env.sh

   # The java implementation to use.
   export JAVA_HOME=/usr

   (b) $HADOOP_CONF_DIR/core-site.xml

   <configuration>
   <property>
     <name>fs.defaultFS</name>
     <value>hdfs://namenode_public_dns:9000</value>
   </property>
   </configuration>

   (c) $HADOOP_CONF_DIR/yarn-site.xml

    <configuration>
    <! — Site specific YARN configuration properties →
    <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value>
    </property> 
    <property>
      <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
      <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <property>
      <name>yarn.resourcemanager.hostname</name>
      <value>namenode_public_dns</value>
    </property>
    </configuration>

   (d) $HADOOP_CONF_DIR/mapred-site.xml

   <configuration>
   <property>
     <name>mapreduce.jobtracker.address</name>
     <value>namenode_public_dns:54311</value>
   </property>
   <property>
     <name>mapreduce.framework.name</name>
     <value>yarn</value>
   </property>
   </configuration>

5. NameNode Specific Configurations

   (a) /etc/hosts

   127.0.0.1 localhost
   namenode_public_dns namenode_hostname
   datanode1_public_dns datanode1_hostname

   (b) $HADOOP_CONF_DIR/hdfs-site.xml

  <configuration>
  <property>
    <name>dfs.replication</name>
    <value>2</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///usr/local/hadoop/hadoop_data/hdfs/namenode</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///usr/local/hadoop/hadoop_data/hdfs/datanode</value>
  </property>
  </configuration>

   (c) $HADOOP_CONF_DIR/masters

       namenode_hostname

   (d) $HADOOP_CONF_DIR/slaves

       namenode_hostname
       datanode_hostname
 
   (e) Create namenode and datanode directories
`
       sudo mkdir -p $HADOOP_HOME/hadoop_data/hdfs/namenode
       sudo mkdir -p $HADOOP_HOME/hadoop_data/hdfs/datanode
       sudo chown -R ubuntu $HADOOP_HOME

6. Datanode Specific Configurations

   (a) $HADOOP_CONF_DIR/hdfs-site.xml

  <configuration>
  <property>
    <name>dfs.replication</name>
    <value>2</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///usr/local/hadoop/hadoop_data/hdfs/namenode</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///usr/local/hadoop/hadoop_data/hdfs/datanode</value>
  </property>
  </configuration>
 
   (b) Create datanode directory
`  
       sudo mkdir -p $HADOOP_HOME/hadoop_data/hdfs/datanode
       sudo chown -R ubuntu $HADOOP_HOME


7. Start Hadoop Cluster

   hdfs namenode -format
   $HADOOP_HOME/sbin/start-dfs.sh
   $HADOOP_HOME/sbin/start-yarn.sh
   $HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver


###########################
Oozie Installation/Build
###########################

Install Maven:

sudo apt-get install maven

Download Oozie-4.1.0

wget http://archive.apache.org/dist/oozie/4.1.0/oozie-4.1.0.tar.gz

Untar

sudo tar xvzf oozie-4.1.0.tar.gz

cd oozie-4.1.0

sudo bin/mkdistro.sh -DskipTests -Dhadoopversion=2.6.0

mkdir ~/oozie-4.1

cp -R distro/target/oozie-4.1.0-distro/oozie-4.1.0/* ~/oozie-4.1

# edit /etc/profile 
sudo vim /etc/profile

    export OOZIE_VERSION=4.1.0
    export OOZIE_HOME=/home/anggao/oozie-4.1
    export PATH=$PATH:$OOZIE_HOME/bin

source /etc/profile

# enable web console for Oozie

# we need ext-*.*.zip library and extjs

cd $OOZIE_HOME

mkdir libext

cp ../oozie-4.1.0/hadooplibs/target/oozie-4.1.0-hadooplibs.tar.gz .

tar -xzvf oozie-4.1.0-hadooplibs.tar.gz

cp oozie-4.1.0/hadooplibs/hadooplib-2.3.0.oozie-4.1.0/* libext

cd libext/

wget http://archive.cloudera.com/gplextras/misc/ext-2.2.zip

# Configure the Hadoop cluster with proxyuser for the Oozie process.
# The following two properties are required in Hadoop core-site.xml:

  <!-- OOZIE -->
  <property>
      <name>hadoop.proxyuser.ubuntu.hosts</name>
      <value>*</value>
  </property>
  <property>
      <name>hadoop.proxyuser.ubuntu.groups</name>
      <value>*</value>
  </property>

## RESTART HADOOP CLUSTER  !! ###

sudo apt-get install unzip

sudo apt-get intall zip

oozie-setup.sh prepare-war

# Create Sharelib Directory on HDFS

# first get HDFS info
hdfs getconf -confKey fs.defaultFS

-->hdfs://ec2-52-91-50-127.compute-1.amazonaws.com:9000

# use the info obtained above
oozie-setup.sh sharelib create -fs hdfs://ec2-52-91-50-127.compute-1.amazonaws.com:9000

  setting CATALINA_OPTS="$CATALINA_OPTS -Xmx1024m"
log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/ubuntu/oozie-4.1/libtools/slf4j-simple-1.6.6.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/ubuntu/oozie-4.1/libtools/slf4j-log4j12-1.6.6.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/ubuntu/oozie-4.1/libext/slf4j-log4j12-1.6.6.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.SimpleLoggerFactory]
the destination path for sharelib is: /user/ubuntu/share/lib/lib_20161209182912


# update oozie-site.xml under OOZIE_CONF_DIR

<property>
        <name>oozie.service.HadoopAccessorService.hadoop.configurations</name>
        <value>*=/usr/local/hadoop/etc/hadoop</value>
        <description>
            Comma separated AUTHORITY=HADOOP_CONF_DIR, where AUTHORITY is the HOST:PORT of
            the Hadoop service (JobTracker, HDFS). The wildcard '*' configuration is
            used when there is no exact match for an authority. The HADOOP_CONF_DIR contains
            the relevant Hadoop *-site.xml files. If the path is relative is looked within
            the Oozie configuration directory; though the path can be absolute (i.e. to point
            to Hadoop client conf/ directories in the local filesystem.
        </description>
    </property>

    <property>
        <name>oozie.service.WorkflowAppService.system.libpath</name>
        <value>/user/ubuntu/share/lib</value>
        <description>
            System library path to use for workflow applications.
            This path is added to workflow application if their job properties sets
            the property 'oozie.use.system.libpath' to true.
        </description>
    </property>


# Create oozie database 
ooziedb.sh create -sqlfile oozie.sql -run

# Start Oozie Service
oozied.sh start

# Verify status of Oozie service
oozie admin --oozie http://localhost:11000/oozie -status

# Compile map reduce java programs

javac -classpath $HADOOP_HOME/share/hadoop/common/hadoop-common-2.6.0.jar:$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.6.0.jar:$HADOOP_HOME/share/hadoop/common/lib/commons-cli-1.2.jar -d ./ *.java

# Copy all *.class files into ~/classfiles

# Create the jar file 

jar -cvf testoozie.jar -C classfiles/ .

# Create ~/map-reduce
# Create ~/map-reduce/lib
# Create input directory /user/ubuntu/input on HDFS

hdfs dfs -mkdir input

# Copy job.properties and workflow.xml into ~/map-reduce
# Copy jar file into ~/map-reduce/lib
# Copy Flight data into HDFS (/user/ubuntu/input)

# Copy ~/map-reduce into HDFS 

hdfs dfs -put ~/map-reduce map-reduce

#run
oozie job -oozie http://localhost:11000/oozie -config ~/map-reduce/job.properties -run


